{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3aa70c",
   "metadata": {},
   "source": [
    "## Fit a \"Regional Grain Size Model\" to 4 regions\n",
    "\n",
    "### SouthEast USA, E Australia, S CA, and whole Australia samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832429a8",
   "metadata": {},
   "source": [
    "Daniel Buscombe, Sept 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90adaf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbuscombe\\Anaconda3\\envs\\cgrasp\\lib\\site-packages\\outdated\\utils.py:14: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.5.1, the latest is 0.5.2.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from pingouin import partial_corr\n",
    "import pingouin as pg\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fbd11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_gs_strat = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066279bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_gs_strat:\n",
    "\n",
    "    df = pd.read_csv('../model_data/dataset_SEUS_EAus_SCA_wholeAus_strat.csv')\n",
    "\n",
    "rand = np.load('../model_out/SEUS_EOz_SCA_wholeAus_rand.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78dc4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1201"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e66a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators':[25,50, 100,200,300, 500], \n",
    "    'max_depth':[3,5,7,9,11], \n",
    "    \"min_samples_split\":[3,5,7,9,11], \n",
    "    \"learning_rate\": [0.01,0.05, 0.075, 0.1], \n",
    "    \"subsample\": [0.25,0.5,0.75,1.0],\n",
    "    \"loss\": [\"squared_error\", \"absolute_error\", \"huber\"]\n",
    "}\n",
    "\n",
    "##27 individual parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138db279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b79df937",
   "metadata": {},
   "outputs": [],
   "source": [
    "Smean = []\n",
    "Sstd = []\n",
    "\n",
    "test_size = 0.6\n",
    "\n",
    "standardize = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfeddbc",
   "metadata": {},
   "source": [
    "### model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c55fa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.942912168\n",
      "1.942912168\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n",
      "The mean squared error (MSE) on test set: 0.0888\n",
      "{'learning_rate': 0.01, 'loss': 'squared_error', 'max_depth': 7, 'min_samples_split': 9, 'n_estimators': 300, 'subsample': 1.0}\n",
      "Mean RMSE: 0.304 (0.034)\n"
     ]
    }
   ],
   "source": [
    "feature_names1 = ['β (radians)', 'Random\\n (non-dim.)']\n",
    "X = np.stack((df['beach_slope_average'], rand))\n",
    "X.shape\n",
    "\n",
    "if standardize:\n",
    "    xscaler = preprocessing.StandardScaler().fit(X)\n",
    "    X = xscaler.transform(X)\n",
    "    print('Covariates standardized')\n",
    "    \n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    X.T, df['d50'], test_size=test_size, random_state=2022\n",
    ")\n",
    "\n",
    "print(y_test1.max())\n",
    "print(y_train1.max())\n",
    "\n",
    "# model1 = ensemble.GradientBoostingRegressor(**params)\n",
    "gbr = ensemble.GradientBoostingRegressor()#**params)\n",
    "\n",
    "model1 = GridSearchCV(gbr, parameters, n_jobs=-1, verbose=2)\n",
    "model1.fit(X_train1, y_train1)\n",
    "\n",
    "mse1 = mean_squared_error(y_test1, model1.predict(X_test1))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse1))\n",
    "\n",
    "print(model1.best_params_)\n",
    "\n",
    "model1out = {}\n",
    "model1out['feature_names'] = feature_names1\n",
    "model1out['Xtest'] = X_test1\n",
    "model1out['Xtrain'] = X_train1\n",
    "model1out['ytest'] = y_test1\n",
    "model1out['ytrain'] = y_train1\n",
    "model1out['model'] = model1\n",
    "model1out['mse'] = mse1\n",
    "model1out['best_params'] = model1.best_params_\n",
    "\n",
    "model1 = model1.best_estimator_\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=2022)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model1, X_test1, y_test1, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = np.absolute(scores)\n",
    "print('Mean RMSE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "Smean.append(scores.mean())\n",
    "Sstd.append(scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a224a",
   "metadata": {},
   "source": [
    "### model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b50c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.942912168\n",
      "1.942912168\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n",
      "The mean squared error (MSE) on test set: 0.0753\n",
      "{'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': 5, 'min_samples_split': 11, 'n_estimators': 50, 'subsample': 1.0}\n",
      "Mean RMSE: 0.275 (0.028)\n"
     ]
    }
   ],
   "source": [
    "feature_names2 = [r'MSTR (m)', 'Random\\n (non-dim.)']\n",
    "X = np.stack((df['mstr'], rand))\n",
    "X.shape\n",
    "\n",
    "if standardize:\n",
    "    xscaler = preprocessing.StandardScaler().fit(X)\n",
    "    X = xscaler.transform(X)\n",
    "    print('Covariates standardized')\n",
    "    \n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X.T, df['d50'], test_size=test_size, random_state=2022\n",
    ")\n",
    "\n",
    "print(y_test2.max())\n",
    "print(y_train2.max())\n",
    "\n",
    "# model2 = ensemble.GradientBoostingRegressor(**params)\n",
    "gbr = ensemble.GradientBoostingRegressor()#**params)\n",
    "\n",
    "model2 = GridSearchCV(gbr, parameters, n_jobs=-1, verbose=2)\n",
    "model2.fit(X_train2, y_train2)\n",
    "\n",
    "mse2 = mean_squared_error(y_test2, model2.predict(X_test2))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse2))\n",
    "\n",
    "print(model2.best_params_)\n",
    "model2out = {}\n",
    "model2out['feature_names'] = feature_names2\n",
    "model2out['Xtest'] = X_test2\n",
    "model2out['Xtrain'] = X_train2\n",
    "model2out['ytest'] = y_test2\n",
    "model2out['ytrain'] = y_train2\n",
    "model2out['model'] = model2\n",
    "model2out['mse'] = mse2\n",
    "model2out['best_params'] = model2.best_params_\n",
    "\n",
    "model2 = model2.best_estimator_\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=2022)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model2, X_test2, y_test2, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = np.absolute(scores)\n",
    "print('Mean RMSE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "Smean.append(scores.mean())\n",
    "Sstd.append(scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03677685",
   "metadata": {},
   "source": [
    "### model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e79b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.942912168\n",
      "1.942912168\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    }
   ],
   "source": [
    "feature_names3 = ['β (radians)','MSTR (m)', 'Random\\n (non-dim.)']\n",
    "X = np.stack((df['beach_slope_average'], df['mstr'], rand))\n",
    "X.shape\n",
    "\n",
    "if standardize:\n",
    "    xscaler = preprocessing.StandardScaler().fit(X)\n",
    "    X = xscaler.transform(X)\n",
    "    print('Covariates standardized')\n",
    "\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(\n",
    "    X.T, df['d50'], test_size=test_size, random_state=2022\n",
    ")\n",
    "\n",
    "print(y_test3.max())\n",
    "print(y_train3.max())\n",
    "\n",
    "# model3 = ensemble.GradientBoostingRegressor(**params)\n",
    "gbr = ensemble.GradientBoostingRegressor()#**params)\n",
    "\n",
    "model3 = GridSearchCV(gbr, parameters, n_jobs=-1, verbose=2)\n",
    "model3.fit(X_train3, y_train3)\n",
    "\n",
    "mse3 = mean_squared_error(y_test3, model3.predict(X_test3))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse3))\n",
    "\n",
    "print(model3.best_params_)\n",
    "model3out = {}\n",
    "model3out['feature_names'] = feature_names3\n",
    "model3out['Xtest'] = X_test3\n",
    "model3out['Xtrain'] = X_train3\n",
    "model3out['ytest'] = y_test3\n",
    "model3out['ytrain'] = y_train3\n",
    "model3out['model'] = model3\n",
    "model3out['mse'] = mse3\n",
    "model3out['best_params'] = model3.best_params_\n",
    "\n",
    "model3 = model3.best_estimator_\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=2022)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model3, X_test3, y_test3, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = np.absolute(scores)\n",
    "print('Mean RMSE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "Smean.append(scores.mean())\n",
    "Sstd.append(scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bb976",
   "metadata": {},
   "source": [
    "### model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names4 = ['β (radians)','MSTR (m)',r'$T_p$ (s)', 'Random\\n (non-dim.)']\n",
    "X = np.stack((df['beach_slope_average'], df['mstr'], df['tp'], rand))\n",
    "X.shape\n",
    "\n",
    "if standardize:\n",
    "    xscaler = preprocessing.StandardScaler().fit(X)\n",
    "    X = xscaler.transform(X)\n",
    "    print('Covariates standardized')\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(\n",
    "    X.T, df['d50'], test_size=test_size, random_state=2022\n",
    ")\n",
    "\n",
    "print(y_test4.max())\n",
    "print(y_train4.max())\n",
    "\n",
    "# model4 = ensemble.GradientBoostingRegressor(**params)\n",
    "gbr = ensemble.GradientBoostingRegressor()#**params)\n",
    "\n",
    "model4 = GridSearchCV(gbr, parameters, n_jobs=-1, verbose=2)\n",
    "model4.fit(X_train4, y_train4)\n",
    "\n",
    "mse4 = mean_squared_error(y_test4, model4.predict(X_test4))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse4))\n",
    "\n",
    "print(model4.best_params_)\n",
    "model4out = {}\n",
    "model4out['feature_names'] = feature_names4\n",
    "model4out['Xtest'] = X_test4\n",
    "model4out['Xtrain'] = X_train4\n",
    "model4out['ytest'] = y_test4\n",
    "model4out['ytrain'] = y_train4\n",
    "model4out['model'] = model4\n",
    "model4out['mse'] = mse4\n",
    "model4out['best_params'] = model4.best_params_\n",
    "\n",
    "model4 = model4.best_estimator_\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=2022)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model4, X_test4, y_test4, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = np.absolute(scores)\n",
    "print('Mean RMSE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "Smean.append(scores.mean())\n",
    "Sstd.append(scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446330a",
   "metadata": {},
   "source": [
    "### model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0caac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names5 = ['β (radians)','MSTR (m)',r'$H_s$ (m)', 'Random\\n (non-dim.)']\n",
    "X = np.stack((df['beach_slope_average'], df['mstr'], df['hs_mean'], rand))\n",
    "X.shape\n",
    "\n",
    "if standardize:\n",
    "    xscaler = preprocessing.StandardScaler().fit(X)\n",
    "    X = xscaler.transform(X)\n",
    "    print('Covariates standardized')\n",
    "\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(\n",
    "    X.T, df['d50'], test_size=test_size, random_state=2022\n",
    ")\n",
    "\n",
    "print(y_test5.max())\n",
    "print(y_train5.max())\n",
    "\n",
    "# model5 = ensemble.GradientBoostingRegressor(**params)\n",
    "gbr = ensemble.GradientBoostingRegressor()#**params)\n",
    "\n",
    "model5 = GridSearchCV(gbr, parameters, n_jobs=-1, verbose=2)\n",
    "model5.fit(X_train5, y_train5)\n",
    "\n",
    "mse5 = mean_squared_error(y_test5, model5.predict(X_test5))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse5))\n",
    "\n",
    "print(model5.best_params_)\n",
    "model5out = {}\n",
    "model5out['feature_names'] = feature_names5\n",
    "model5out['Xtest'] = X_test5\n",
    "model5out['Xtrain'] = X_train5\n",
    "model5out['ytest'] = y_test5\n",
    "model5out['ytrain'] = y_train5\n",
    "model5out['model'] = model5\n",
    "model5out['mse'] = mse5\n",
    "model5out['best_params'] = model5.best_params_\n",
    "\n",
    "model5 = model5.best_estimator_\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=2022)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model5, X_test5, y_test5, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = np.absolute(scores)\n",
    "print('Mean RMSE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "Smean.append(scores.mean())\n",
    "Sstd.append(scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d2f88",
   "metadata": {},
   "source": [
    "### model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names6 = ['β (radians)','MSTR (m)',r'$\\theta$ (deg.)', 'Random\\n (non-dim.)']\n",
    "X = np.stack((df['beach_slope_average'], df['mstr'], df['dir'], rand))\n",
    "X.shape\n",
    "\n",
    "if standardize:\n",
    "    xscaler = preprocessing.StandardScaler().fit(X)\n",
    "    X = xscaler.transform(X)\n",
    "    print('Covariates standardized')\n",
    "\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(\n",
    "    X.T, df['d50'], test_size=test_size, random_state=2022\n",
    ")\n",
    "\n",
    "print(y_test6.max())\n",
    "print(y_train6.max())\n",
    "\n",
    "# model6 = ensemble.GradientBoostingRegressor(**params)\n",
    "gbr = ensemble.GradientBoostingRegressor()#**params)\n",
    "\n",
    "model6 = GridSearchCV(gbr, parameters, n_jobs=-1, verbose=2)\n",
    "model6.fit(X_train6, y_train6)\n",
    "\n",
    "mse6 = mean_squared_error(y_test6,model6.predict(X_test6))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse6))\n",
    "\n",
    "print(model6.best_params_)\n",
    "\n",
    "model6out = {}\n",
    "model6out['feature_names'] = feature_names6\n",
    "model6out['Xtest'] = X_test6\n",
    "model6out['Xtrain'] = X_train6\n",
    "model6out['ytest'] = y_test6\n",
    "model6out['ytrain'] = y_train6\n",
    "model6out['model'] = model6\n",
    "model6out['mse'] = mse6\n",
    "model6out['best_params'] = model6.best_params_\n",
    "\n",
    "model6 = model6.best_estimator_\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=2022)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model6, X_test6, y_test6, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = np.absolute(scores)\n",
    "print('Mean RMSE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "Smean.append(scores.mean())\n",
    "Sstd.append(scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cdb9c9",
   "metadata": {},
   "source": [
    "### model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db1755",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names7 = ['MSTR (m)',r'$T_p$ (s)',r'$H_s$ (m)', 'Random\\n (non-dim.)']\n",
    "X = np.stack((df['mstr'], df['tp'], df['hs_mean'],  rand))\n",
    "X.shape\n",
    "\n",
    "if standardize:\n",
    "    xscaler = preprocessing.StandardScaler().fit(X)\n",
    "    X = xscaler.transform(X)\n",
    "    print('Covariates standardized')\n",
    "\n",
    "X_train7, X_test7, y_train7, y_test7 = train_test_split(\n",
    "    X.T, df['d50'], test_size=test_size, random_state=2022\n",
    ")\n",
    "\n",
    "print(y_test7.max())\n",
    "print(y_train7.max())\n",
    "\n",
    "# model7 = ensemble.GradientBoostingRegressor(**params)\n",
    "gbr = ensemble.GradientBoostingRegressor()#**params)\n",
    "\n",
    "model7 = GridSearchCV(gbr, parameters, n_jobs=-1, verbose=2)\n",
    "model7.fit(X_train7, y_train7)\n",
    "\n",
    "mse7 = mean_squared_error(y_test7,model7.predict(X_test7))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse7))\n",
    "\n",
    "print(model7.best_params_)\n",
    "\n",
    "model7out = {}\n",
    "model7out['feature_names'] = feature_names7\n",
    "model7out['Xtest'] = X_test7\n",
    "model7out['Xtrain'] = X_train7\n",
    "model7out['ytest'] = y_test7\n",
    "model7out['ytrain'] = y_train7\n",
    "model7out['model'] = model7\n",
    "model7out['mse'] = mse7\n",
    "model7out['best_params'] = model7.best_params_\n",
    "\n",
    "model7 = model7.best_estimator_\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=2022)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model7, X_test7, y_test7, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = np.absolute(scores)\n",
    "print('Mean RMSE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "Smean.append(scores.mean())\n",
    "Sstd.append(scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be59cdf",
   "metadata": {},
   "source": [
    "### model 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c851a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names8 = ['β (radians)','MSTR (m)',r'$T_p$ (s)',r'$H_s$ (m)',  'Random\\n (non-dim.)']\n",
    "X = np.stack((df['beach_slope_average'], df['mstr'], df['tp'], df['hs_mean'],  rand))\n",
    "X.shape\n",
    "\n",
    "if standardize:\n",
    "    xscaler = preprocessing.StandardScaler().fit(X)\n",
    "    X = xscaler.transform(X)\n",
    "    print('Covariates standardized')\n",
    "\n",
    "X_train8, X_test8, y_train8, y_test8= train_test_split(\n",
    "    X.T, df['d50'], test_size=test_size, random_state=2022\n",
    ")\n",
    "\n",
    "print(y_test8.max())\n",
    "print(y_train8.max())\n",
    "\n",
    "# model8 = ensemble.GradientBoostingRegressor(**params)\n",
    "gbr = ensemble.GradientBoostingRegressor()#**params)\n",
    "\n",
    "model8 = GridSearchCV(gbr, parameters, n_jobs=-1, verbose=2)\n",
    "model8.fit(X_train8, y_train8)\n",
    "\n",
    "mse8 = mean_squared_error(y_test8,model8.predict(X_test8))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse8))\n",
    "\n",
    "print(model8.best_params_)\n",
    "\n",
    "model8out = {}\n",
    "model8out['feature_names'] = feature_names8\n",
    "model8out['Xtest'] = X_test8\n",
    "model8out['Xtrain'] = X_train8\n",
    "model8out['ytest'] = y_test8\n",
    "model8out['ytrain'] = y_train8\n",
    "model8out['model'] = model8\n",
    "model8out['mse'] = mse8\n",
    "model8out['best_params'] = model8.best_params_\n",
    "\n",
    "model8 = model8.best_estimator_\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=2022)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model8, X_test8, y_test8, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = np.absolute(scores)\n",
    "print('Mean RMSE: %.3f (%.3f)' % (scores.mean(), scores.std()) )\n",
    "\n",
    "Smean.append(scores.mean())\n",
    "Sstd.append(scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd54285",
   "metadata": {},
   "source": [
    "### plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prc_err(y,yest):\n",
    "    return 100*(np.abs(y-yest)/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "plt.subplot(241)\n",
    "ds_est = model1.predict(X_test1)\n",
    "plt.plot(y_test1, ds_est, 'ko', label='Test data')\n",
    "plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "yl=plt.ylim()\n",
    "plt.plot(yl,yl,'--r')\n",
    "plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "plt.title(r\"a) f(β)\", loc='left')\n",
    "r2 = np.min(np.corrcoef(y_test1, ds_est))**2\n",
    "plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "coef = np.polyfit(y_test1, ds_est,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "plt.plot(y_test1, poly1d_fn(y_test1), '-k')\n",
    "plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test1,ds_est)))[:2]))\n",
    "plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[0])[:4]))\n",
    "\n",
    "ds_est2 = model1.predict(X_train1)\n",
    "plt.plot(y_train1, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test1,y_train1)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "plt.subplot(242)\n",
    "ds_est = model2.predict(X_test2)\n",
    "plt.plot(y_test2, ds_est, 'ko')\n",
    "plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "yl=plt.ylim()\n",
    "plt.plot(yl,yl,'--r')\n",
    "plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "plt.title(r\"b) f($MSTR$)\", loc='left')\n",
    "r2 = np.min(np.corrcoef(y_test2, ds_est))**2\n",
    "plt.text(1.2,.5,r'$R^2$='+str(r2)[:4])\n",
    "\n",
    "coef = np.polyfit(y_test2, ds_est,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "plt.plot(y_test2, poly1d_fn(y_test2), '-k')\n",
    "plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test2,ds_est)))[:2]))\n",
    "plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[1])[:4]))\n",
    "\n",
    "ds_est2 = model2.predict(X_train2)\n",
    "plt.plot(y_train2, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test2,y_train2)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "plt.subplot(243)\n",
    "ds_est = model3.predict(X_test3)\n",
    "plt.plot(y_test3, ds_est, 'ko')\n",
    "plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "yl=plt.ylim()\n",
    "plt.plot(yl,yl,'--r')\n",
    "plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "plt.title(\"c) f(β, $MSTR$)\", loc='left')\n",
    "r2 = np.min(np.corrcoef(y_test3, ds_est))**2\n",
    "plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "coef = np.polyfit(y_test3, ds_est,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "plt.plot(y_test3, poly1d_fn(y_test3), '-k')\n",
    "plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test3,ds_est)))[:2]))\n",
    "plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[2])[:4]))\n",
    "\n",
    "ds_est2 = model3.predict(X_train3)\n",
    "plt.plot(y_train3, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test3,y_train3)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "plt.subplot(244)\n",
    "ds_est = model4.predict(X_test4)\n",
    "plt.plot(y_test4, ds_est, 'ko')\n",
    "plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "yl=plt.ylim()\n",
    "plt.plot(yl,yl,'--r')\n",
    "plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "plt.title(r\"d) f(β, $MSTR$, $T_p$)\", loc='left')\n",
    "r2 = np.min(np.corrcoef(y_test4, ds_est))**2\n",
    "plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test4,ds_est)))[:2]))\n",
    "plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[3])[:4]))\n",
    "\n",
    "coef = np.polyfit(y_test4, ds_est,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "plt.plot(y_test4, poly1d_fn(y_test4), '-k')\n",
    "\n",
    "ds_est2 = model4.predict(X_train4)\n",
    "plt.plot(y_train4, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test4,y_train4)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "\n",
    "plt.subplot(245)\n",
    "ds_est = model5.predict(X_test5)\n",
    "plt.plot(y_test5, ds_est, 'ko')\n",
    "plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "yl=plt.ylim()\n",
    "plt.plot(yl,yl,'--r')\n",
    "plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "plt.title(r\"e) f(β, $MSTR$, $H_s$)\", loc='left')\n",
    "r2 = np.min(np.corrcoef(y_test5, ds_est))**2\n",
    "plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "coef = np.polyfit(y_test5, ds_est,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "plt.plot(y_test5, poly1d_fn(y_test5), '-k')\n",
    "plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test5,ds_est)))[:2]))\n",
    "plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[4])[:4]))\n",
    "\n",
    "ds_est2 = model5.predict(X_train5)\n",
    "plt.plot(y_train5, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test5,y_train5)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "\n",
    "plt.subplot(246)\n",
    "ds_est = model6.predict(X_test6)\n",
    "plt.plot(y_test6, ds_est, 'ko')\n",
    "plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "yl=plt.ylim()\n",
    "plt.plot(yl,yl,'--r')\n",
    "plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "plt.title(r\"f) f(β, $MSTR$, $\\theta$)\", loc='left')\n",
    "r2 = np.min(np.corrcoef(y_test6, ds_est))**2\n",
    "plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "coef = np.polyfit(y_test6, ds_est,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "plt.plot(y_test6, poly1d_fn(y_test6), '-k')\n",
    "plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test6,ds_est)))[:2]))\n",
    "plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[5])[:4]))\n",
    "\n",
    "ds_est2 = model6.predict(X_train6)\n",
    "plt.plot(y_train6, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test6,y_train6)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "plt.subplot(247)\n",
    "ds_est = model7.predict(X_test7)\n",
    "plt.plot(y_test7, ds_est, 'ko')\n",
    "plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "yl=plt.ylim()\n",
    "plt.plot(yl,yl,'--r')\n",
    "plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "plt.title(r\"g) f($MSTR$, $T_p$, $H_s$)\", loc='left')\n",
    "r2 = np.min(np.corrcoef(y_test7, ds_est))**2\n",
    "plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "coef = np.polyfit(y_test7, ds_est,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "plt.plot(y_test7, poly1d_fn(y_test7), '-k')\n",
    "plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test7,ds_est)))[:2]))\n",
    "plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[6])[:4]))\n",
    "\n",
    "ds_est2 = model7.predict(X_train7)\n",
    "plt.plot(y_train7, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test7,y_train7)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "plt.subplot(248)\n",
    "ds_est = model8.predict(X_test8)\n",
    "plt.plot(y_test8, ds_est, 'ko')\n",
    "plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "yl=plt.ylim()\n",
    "plt.plot(yl,yl,'--r')\n",
    "plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "plt.title(r\"h) f(β, $MSTR$, $T_p$, $H_s$)\", loc='left')\n",
    "r2 = np.min(np.corrcoef(y_test8, ds_est))**2\n",
    "plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "coef = np.polyfit(y_test8, ds_est,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "plt.plot(y_test8, poly1d_fn(y_test8), '-k')\n",
    "plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test8,ds_est)))[:2]))\n",
    "plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[7])[:4]))\n",
    "\n",
    "ds_est2 = model8.predict(X_train8)\n",
    "plt.plot(y_train8, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test8,y_train8)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "#\n",
    "\n",
    "plt.savefig('../model_plots/FourRegions_SEUS_EAus_SCA_wholeAus_d50-8models-skill-FINAL.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22516d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "# plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "# plt.subplot(241)\n",
    "# ds_est = model1.predict(X_test1)\n",
    "# plt.loglog(y_test1, ds_est, 'ko')\n",
    "\n",
    "# plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "# yl=plt.ylim()\n",
    "# plt.plot(yl,yl,'--r')\n",
    "# plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "# plt.title(r\"a) f(β)\", loc='left')\n",
    "# r2 = np.min(np.corrcoef(y_test1, ds_est))**2\n",
    "# plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "# # coef = np.polyfit(y_test1, ds_est,1)\n",
    "# # poly1d_fn = np.poly1d(coef) \n",
    "# # plt.plot(y_test1, poly1d_fn(y_test1), '-k')\n",
    "# # plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test1,ds_est)))[:2]))\n",
    "# # plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[0])[:4]))\n",
    "\n",
    "# # ds_est2 = model1.predict(X_train1)\n",
    "# # plt.plot(y_train1, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "# # plt.legend(loc=0)\n",
    "\n",
    "# # plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "# # plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test1,y_train1)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "# plt.subplot(242)\n",
    "# ds_est = model2.predict(X_test2)\n",
    "# plt.loglog(y_test2, ds_est, 'ko')\n",
    "# plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "# yl=plt.ylim()\n",
    "# plt.plot(yl,yl,'--r')\n",
    "# plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# # plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "# plt.title(r\"b) f($MSTR$)\", loc='left')\n",
    "# r2 = np.min(np.corrcoef(y_test2, ds_est))**2\n",
    "# plt.text(1.2,.5,r'$R^2$='+str(r2)[:4])\n",
    "\n",
    "# # coef = np.polyfit(y_test2, ds_est,1)\n",
    "# # poly1d_fn = np.poly1d(coef) \n",
    "# # plt.plot(y_test2, poly1d_fn(y_test2), '-k')\n",
    "# # plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test2,ds_est)))[:2]))\n",
    "# # plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[1])[:4]))\n",
    "\n",
    "# # ds_est2 = model2.predict(X_train2)\n",
    "# # plt.plot(y_train2, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "# # plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "# # plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test2,y_train2)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "# plt.subplot(243)\n",
    "# ds_est = model3.predict(X_test3)\n",
    "# plt.loglog(y_test3, ds_est, 'ko')\n",
    "# plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "# yl=plt.ylim()\n",
    "# plt.plot(yl,yl,'--r')\n",
    "# plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# # plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "# plt.title(\"c) f(β, $MSTR$)\", loc='left')\n",
    "# r2 = np.min(np.corrcoef(y_test3, ds_est))**2\n",
    "# plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "# # coef = np.polyfit(y_test3, ds_est,1)\n",
    "# # poly1d_fn = np.poly1d(coef) \n",
    "# # plt.plot(y_test3, poly1d_fn(y_test3), '-k')\n",
    "# # plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test3,ds_est)))[:2]))\n",
    "# # plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[2])[:4]))\n",
    "\n",
    "# # ds_est2 = model3.predict(X_train3)\n",
    "# # plt.plot(y_train3, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "# # plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "# # plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test3,y_train3)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "# plt.subplot(244)\n",
    "# ds_est = model4.predict(X_test4)\n",
    "# plt.loglog(y_test4, ds_est, 'ko')\n",
    "# plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "# yl=plt.ylim()\n",
    "# plt.plot(yl,yl,'--r')\n",
    "# plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# plt.title(r\"d) f(β, $MSTR$, $T_p$)\", loc='left')\n",
    "# r2 = np.min(np.corrcoef(y_test4, ds_est))**2\n",
    "# plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "# # coef = np.polyfit(y_test4, ds_est,1)\n",
    "# # poly1d_fn = np.poly1d(coef) \n",
    "# # plt.plot(y_test4, poly1d_fn(y_test4), '-k')\n",
    "# # plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test4,ds_est)))[:2]))\n",
    "# # plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[3])[:4]))\n",
    "\n",
    "# # ds_est2 = model4.predict(X_train4)\n",
    "# # plt.plot(y_train4, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "# # plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "# # plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test4,y_train4)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "\n",
    "# plt.subplot(245)\n",
    "# ds_est = model5.predict(X_test5)\n",
    "# plt.loglog(y_test5, ds_est, 'ko')\n",
    "# plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "# yl=plt.ylim()\n",
    "# plt.plot(yl,yl,'--r')\n",
    "# plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "# plt.title(r\"e) f(β, $MSTR$, $H_s$)\", loc='left')\n",
    "# r2 = np.min(np.corrcoef(y_test5, ds_est))**2\n",
    "# plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "# # coef = np.polyfit(y_test5, ds_est,1)\n",
    "# # poly1d_fn = np.poly1d(coef) \n",
    "# # plt.plot(y_test5, poly1d_fn(y_test5), '-k')\n",
    "# # plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test5,ds_est)))[:2]))\n",
    "# # plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[4])[:4]))\n",
    "\n",
    "# # ds_est2 = model5.predict(X_train5)\n",
    "# # plt.plot(y_train5, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "# # plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "# # plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test5,y_train5)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "\n",
    "# plt.subplot(246)\n",
    "# ds_est = model6.predict(X_test6)\n",
    "# plt.loglog(y_test6, ds_est, 'ko')\n",
    "# plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "# yl=plt.ylim()\n",
    "# plt.plot(yl,yl,'--r')\n",
    "# plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# # plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "# plt.title(r\"f) f(β, $MSTR$, $\\theta$)\", loc='left')\n",
    "# r2 = np.min(np.corrcoef(y_test6, ds_est))**2\n",
    "# plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "# # coef = np.polyfit(y_test6, ds_est,1)\n",
    "# # poly1d_fn = np.poly1d(coef) \n",
    "# # plt.plot(y_test6, poly1d_fn(y_test6), '-k')\n",
    "# # plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test6,ds_est)))[:2]))\n",
    "# # plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[5])[:4]))\n",
    "\n",
    "# # ds_est2 = model6.predict(X_train6)\n",
    "# # plt.plot(y_train6, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "# # plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "# # plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test6,y_train6)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "# plt.subplot(247)\n",
    "# ds_est = model7.predict(X_test7)\n",
    "# plt.loglog(y_test7, ds_est, 'ko')\n",
    "# plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "\n",
    "# yl=plt.ylim()\n",
    "# plt.plot(yl,yl,'--r')\n",
    "# plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# # plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "# plt.title(r\"g) f($MSTR$, $T_p$, $H_s$)\", loc='left')\n",
    "# r2 = np.min(np.corrcoef(y_test7, ds_est))**2\n",
    "# plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "# # coef = np.polyfit(y_test7, ds_est,1)\n",
    "# # poly1d_fn = np.poly1d(coef) \n",
    "# # plt.plot(y_test7, poly1d_fn(y_test7), '-k')\n",
    "# # plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test7,ds_est)))[:2]))\n",
    "# # plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[6])[:4]))\n",
    "\n",
    "# # ds_est2 = model7.predict(X_train7)\n",
    "# # plt.plot(y_train7, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "# # plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "# # plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test7,y_train7)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "\n",
    "# plt.subplot(248)\n",
    "# ds_est = model8.predict(X_test8)\n",
    "# plt.loglog(y_test8, ds_est, 'ko')\n",
    "# plt.xlim(0.063,2.0); plt.ylim(0.063,2.0)\n",
    "# yl=plt.ylim()\n",
    "# plt.plot(yl,yl,'--r')\n",
    "# plt.xlabel(r'Observed $D_{50}$ (mm)')\n",
    "# # plt.ylabel(r'Estimated $D_{50}$ (mm)')\n",
    "# plt.title(r\"h) f(β, $MSTR$, $T_p$, $H_s$)\", loc='left')\n",
    "# r2 = np.min(np.corrcoef(y_test8, ds_est))**2\n",
    "# plt.text(1.2,.5,r'$R^2$ (test)='+str(r2)[:4])\n",
    "\n",
    "# # coef = np.polyfit(y_test8, ds_est,1)\n",
    "# # poly1d_fn = np.poly1d(coef) \n",
    "# # plt.plot(y_test8, poly1d_fn(y_test8), '-k')\n",
    "# # plt.text(1.2,.4, r'Mean error: {}%'.format(str(np.mean(prc_err(y_test8,ds_est)))[:2]))\n",
    "# # plt.text(1.2,.3, r'MSE: {}mm'.format(str(Smean[7])[:4]))\n",
    "\n",
    "# # ds_est2 = model8.predict(X_train8)\n",
    "# # plt.plot(y_train8, ds_est2, 'bp', alpha=0.5, label='Train data')\n",
    "\n",
    "# # plt.text(1.2,.2, r'Overall mean error:', color='g')\n",
    "# # plt.text(1.2,.1, r'{}%'.format(str(np.mean(prc_err(np.hstack((y_test8,y_train8)),np.hstack((ds_est,ds_est2)))))[:2]), color='g')\n",
    "# # #\n",
    "\n",
    "# # plt.savefig('../model_plots/ThreeRegions_SEUS_EAus_SCA_d50-8models-skill-FINAL.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc3ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb09d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack((df['beach_slope_average'], df['mstr'], df['tp'], df['hs_mean'],  rand))\n",
    "ds_est8 = model8.predict(X.T)\n",
    "\n",
    "X = np.stack((df['beach_slope_average'], df['mstr'], df['tp'], rand))\n",
    "ds_est4 = model4.predict(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.stem(df['d50'], df['d50']-ds_est8,  markerfmt='o', linefmt='-', label=r\"f(β, $MSTR$, $T_p$, $H_s$)\")\n",
    "plt.stem(df['d50'], df['d50']-ds_est4,  markerfmt='p', linefmt='-', label = r\"f(β, $MSTR$, $T_p$)\" )\n",
    "plt.legend()\n",
    "plt.ylabel(r'Model residuals')\n",
    "plt.xlabel(r'$D_{50}$ (mm)')\n",
    "plt.title(r\"a) \", loc='left')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.stem(df['hs_mean'], df['d50']-ds_est8,  markerfmt='o', linefmt='-', label=r\"f(β, $MSTR$, $T_p$, $H_s$)\")\n",
    "plt.stem(df['hs_mean'], df['d50']-ds_est4,  markerfmt='p', linefmt='-', label = r\"f(β, $MSTR$, $T_p$)\" )\n",
    "plt.legend()\n",
    "plt.ylabel(r'Model residuals')\n",
    "plt.xlabel(r'$H_s$ (m)')\n",
    "plt.title(r\"b) \", loc='left')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.stem(df['tp'], df['d50']-ds_est8,  markerfmt='o', linefmt='-', label=r\"f(β, $MSTR$, $T_p$, $H_s$)\")\n",
    "plt.stem(df['tp'], df['d50']-ds_est4,  markerfmt='p', linefmt='-', label = r\"f(β, $MSTR$, $T_p$)\" )\n",
    "plt.legend()\n",
    "plt.ylabel(r'Model residuals')\n",
    "plt.xlabel(r'$T_p$ (m)')\n",
    "plt.title(r\"c) \", loc='left')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.stem(df['mstr'], df['d50']-ds_est8,  markerfmt='o', linefmt='-', label=r\"f(β, $MSTR$, $T_p$, $H_s$)\")\n",
    "plt.stem(df['mstr'], df['d50']-ds_est4,  markerfmt='p', linefmt='-', label = r\"f(β, $MSTR$, $T_p$)\" )\n",
    "plt.legend()\n",
    "plt.ylabel(r'Model residuals')\n",
    "plt.xlabel(r'$MSTR$ (m)')\n",
    "plt.title(r\"d) \", loc='left')\n",
    "\n",
    "plt.savefig('../model_plots/FourRegions_SEUS_EAus_SCA_wholeAus_d50-model8_residuals-skill-FINAL.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b604f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b011f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = df['d50'].mean()\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(20,10))\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "####################\n",
    "test_score = np.zeros((model1.n_estimators,), dtype=np.float64)\n",
    "for i, y_pred1 in enumerate(model1.staged_predict(X_test1)):\n",
    "    test_score[i] = model1.loss_(y_test1, y_pred1)\n",
    "\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.title(r\"a) f(β)\", loc='left')\n",
    "plt.plot(\n",
    "    np.arange(model1.n_estimators) + 1,\n",
    "    100*(np.sqrt(model1.train_score_)/mm),\n",
    "    \"b-\",\n",
    "    label=\"Training set\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(model1.n_estimators) + 1, 100*(np.sqrt(test_score)/mm), \"r--\", label=\"Test set\", lw=2\n",
    ")\n",
    "plt.axhline(20, linestyle=':',label='20% threshold')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting iterations\")\n",
    "plt.ylabel(\"Mean deviance (%)\")\n",
    "plt.ylim(0,75)\n",
    "plt.legend(loc=0)\n",
    "\n",
    "###########################\n",
    "test_score = np.zeros((model2.n_estimators,), dtype=np.float64)\n",
    "for i, y_pred2 in enumerate(model2.staged_predict(X_test2)):\n",
    "    test_score[i] = model2.loss_(y_test2, y_pred2)\n",
    "\n",
    "plt.subplot(2,4, 2)\n",
    "plt.title(r\"b) f($MSTR$)\", loc='left')\n",
    "plt.plot(\n",
    "    np.arange(model2.n_estimators) + 1,\n",
    "    100*(np.sqrt(model2.train_score_)/mm),\n",
    "    \"b-\",\n",
    "    label=\"Training set\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(model2.n_estimators) + 1, 100*(np.sqrt(test_score)/mm), \"r--\", label=\"Test set\", lw=2\n",
    ")\n",
    "plt.axhline(20, linestyle=':',label='20% threshold')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting iterations\")\n",
    "# plt.ylabel(\"Deviance (non-dim.)\")\n",
    "plt.ylim(0,75)\n",
    "\n",
    "#############################\n",
    "test_score = np.zeros((model3.n_estimators,), dtype=np.float64)\n",
    "for i, y_pred3 in enumerate(model3.staged_predict(X_test3)):\n",
    "    test_score[i] = model3.loss_(y_test3, y_pred3)\n",
    "\n",
    "plt.subplot(2,4, 3)\n",
    "plt.title(\"c) f(β, $MSTR$)\", loc='left')\n",
    "plt.plot(\n",
    "    np.arange(model3.n_estimators) + 1,\n",
    "    100*(np.sqrt(model3.train_score_)/mm),\n",
    "    \"b-\",\n",
    "    label=\"Training set\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(model3.n_estimators) + 1, 100*(np.sqrt(test_score)/mm), \"r--\", label=\"Test set\", lw=2\n",
    ")\n",
    "plt.axhline(20, linestyle=':',label='20% threshold')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting iterations\")\n",
    "# plt.ylabel(\"Deviance (non-dim.)\")\n",
    "plt.ylim(0,75)\n",
    "\n",
    "#####################\n",
    "test_score = np.zeros((model4.n_estimators,), dtype=np.float64)\n",
    "for i, y_pred4 in enumerate(model4.staged_predict(X_test4)):\n",
    "    test_score[i] = model4.loss_(y_test4, y_pred4)\n",
    "    \n",
    "plt.subplot(2, 4, 4)\n",
    "plt.title(r\"d) f(β, $MSTR$, $T_p$) \", loc='left')\n",
    "plt.plot(\n",
    "    np.arange(model4.n_estimators) + 1,\n",
    "    100*(np.sqrt(model4.train_score_)/mm),\n",
    "    \"b-\",\n",
    "    label=\"Training set\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(model4.n_estimators) + 1, 100*(np.sqrt(test_score)/mm), \"r--\", label=\"Test set\", lw=2\n",
    ")\n",
    "plt.axhline(20, linestyle=':',label='20% threshold')\n",
    "plt.xlabel(\"Boosting iterations\")\n",
    "# plt.ylabel(\"Deviance (non-dim.)\")\n",
    "plt.ylim(0,75)\n",
    "\n",
    "#####################\n",
    "test_score = np.zeros((model5.n_estimators,), dtype=np.float64)\n",
    "for i, y_pred5 in enumerate(model5.staged_predict(X_test5)):\n",
    "    test_score[i] = model5.loss_(y_test5, y_pred5)\n",
    "    \n",
    "plt.subplot(2,4, 5)\n",
    "plt.title(r\"e) f(β, $MSTR$, $H_s$) \", loc='left')\n",
    "plt.plot(\n",
    "    np.arange(model5.n_estimators) + 1,\n",
    "    100*(np.sqrt(model5.train_score_)/mm),\n",
    "    \"b-\",\n",
    "    label=\"Training set\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(model5.n_estimators) + 1, 100*(np.sqrt(test_score)/mm), \"r--\", label=\"Test set\", lw=2\n",
    ")\n",
    "plt.axhline(20, linestyle=':',label='20% threshold')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting iterations\")\n",
    "plt.ylabel(\"Mean deviance (%)\")\n",
    "plt.ylim(0,75)\n",
    "\n",
    "#####################\n",
    "test_score = np.zeros((model6.n_estimators,), dtype=np.float64)\n",
    "for i, y_pred6 in enumerate(model6.staged_predict(X_test6)):\n",
    "    test_score[i] = model6.loss_(y_test6, y_pred6)\n",
    "    \n",
    "plt.subplot(2, 4, 6)\n",
    "plt.title(r\"f) f(β, $MSTR$, $\\theta$) \", loc='left')\n",
    "plt.plot(\n",
    "    np.arange(model6.n_estimators) + 1,\n",
    "    100*(np.sqrt(model6.train_score_)/mm),\n",
    "    \"b-\",\n",
    "    label=\"Training set\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(model6.n_estimators) + 1, 100*(np.sqrt(test_score)/mm), \"r--\", label=\"Test set\", lw=2\n",
    ")\n",
    "plt.axhline(20, linestyle=':',label='20% threshold')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting iterations\")\n",
    "# plt.ylabel(\"Deviance (non-dim.)\")\n",
    "plt.ylim(0,75)\n",
    "\n",
    "#####################\n",
    "test_score = np.zeros((model7.n_estimators,), dtype=np.float64)\n",
    "for i, y_pred7 in enumerate(model7.staged_predict(X_test7)):\n",
    "    test_score[i] = model7.loss_(y_test7, y_pred7)\n",
    "    \n",
    "plt.subplot(2, 4, 7)\n",
    "plt.title(r\"g) f($MSTR$, $T_p$, $H_s$) \", loc='left')\n",
    "plt.plot(\n",
    "    np.arange(model7.n_estimators) + 1,\n",
    "    100*(np.sqrt(model7.train_score_)/mm),\n",
    "    \"b-\",\n",
    "    label=\"Training set\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(model7.n_estimators) + 1, 100*(np.sqrt(test_score)/mm), \"r--\", label=\"Test set\", lw=2\n",
    ")\n",
    "plt.axhline(20, linestyle=':',label='20% threshold')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting iterations\")\n",
    "# plt.ylabel(\"Deviance (non-dim.)\")\n",
    "plt.ylim(0,75)\n",
    "\n",
    "#####################\n",
    "test_score = np.zeros((model8.n_estimators,), dtype=np.float64)\n",
    "for i, y_pred8 in enumerate(model8.staged_predict(X_test8)):\n",
    "    test_score[i] = model8.loss_(y_test8, y_pred8)\n",
    "    \n",
    "plt.subplot(2, 4, 8)\n",
    "plt.title(r\"h) f(β, $MSTR$, $T_p$, $H_s$) \", loc='left')\n",
    "plt.plot(\n",
    "    np.arange(model8.n_estimators) + 1,\n",
    "    100*(np.sqrt(model8.train_score_)/mm),\n",
    "    \"b-\",\n",
    "    label=\"Training set\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(model8.n_estimators) + 1, 100*(np.sqrt(test_score)/mm), \"r--\", label=\"Test set\", lw=2\n",
    ")\n",
    "plt.axhline(20, linestyle=':',label='20% threshold')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting iterations\")\n",
    "# plt.ylabel(\"Deviance (non-dim.)\")\n",
    "plt.ylim(0,75)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('../model_plots/FourRegions_SEUS_EAus_SCA_wholeAus_d50-8models-training-FINAL.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58fc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig=plt.figure(figsize=(12,8))\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "########################\n",
    "result = permutation_importance(\n",
    "    model1, X_test1, y_test1, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "ax=plt.subplot(241)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(feature_names1)[sorted_idx],\n",
    ")\n",
    "\n",
    "plt.xlim(0,2)\n",
    "plt.title(\"a) f(β) \", loc='left')\n",
    "plt.xlabel(\"Permutation importance\\n (non-dim.), test set\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.setp(ax.get_yticklabels (), rotation=30)\n",
    "\n",
    "#########################\n",
    "result = permutation_importance(\n",
    "    model2, X_test2, y_test2, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "ax=plt.subplot(242)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(feature_names2)[sorted_idx],\n",
    ")\n",
    "\n",
    "plt.xlim(0,2)\n",
    "plt.title(\"b) f($MSTR$) \", loc='left')\n",
    "# plt.xlabel(\"Permutation importance (non-dim.), test set\")\n",
    "# plt.ylabel(\"Variable\")\n",
    "plt.setp(ax.get_yticklabels (), rotation=30)\n",
    "\n",
    "#############################\n",
    "result = permutation_importance(\n",
    "    model3, X_test3, y_test3, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "ax=plt.subplot(243)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(feature_names3)[sorted_idx],\n",
    ")\n",
    "\n",
    "plt.xlim(0,2)\n",
    "plt.title(\"c) f(β + $MSTR$) \", loc='left')\n",
    "# plt.xlabel(\"Permutation importance (non-dim.), test set\")\n",
    "# plt.ylabel(\"Variable\")\n",
    "plt.setp(ax.get_yticklabels (), rotation=30)\n",
    "\n",
    "############################\n",
    "result = permutation_importance(\n",
    "    model4, X_test4, y_test4, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "ax=plt.subplot(244)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(feature_names4)[sorted_idx],\n",
    ")\n",
    "plt.xlim(0,2)\n",
    "plt.title(r\"d) f(β, $MSTR$, $T_p$)\", loc='left')\n",
    "# plt.xlabel(\"Permutation importance (non-dim.), test set\")\n",
    "# plt.ylabel(\"Variable\")\n",
    "plt.setp(ax.get_yticklabels (), rotation=30)\n",
    "\n",
    "############################\n",
    "result = permutation_importance(\n",
    "    model5, X_test5, y_test5, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "ax=plt.subplot(245)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(feature_names5)[sorted_idx],\n",
    ")\n",
    "plt.xlim(0,2)\n",
    "plt.title(r\"e) f(β, $MSTR$, $H_s$)\", loc='left')\n",
    "plt.xlabel(\"Permutation importance\\n (non-dim.), test set\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.setp(ax.get_yticklabels (), rotation=30)\n",
    "\n",
    "##################################\n",
    "result = permutation_importance(\n",
    "    model6, X_test6, y_test6, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "ax=plt.subplot(246)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(feature_names6)[sorted_idx],\n",
    ")\n",
    "plt.xlim(0,2)\n",
    "plt.title(r\"f) f(β, $MSTR$, $\\theta$)\", loc='left')\n",
    "# plt.xlabel(\"Permutation importance (non-dim.), test set\")\n",
    "# plt.ylabel(\"Variable\")\n",
    "plt.setp(ax.get_yticklabels (), rotation=30)\n",
    "\n",
    "##################################\n",
    "result = permutation_importance(\n",
    "    model7, X_test7, y_test7, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "ax=plt.subplot(247)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(feature_names7)[sorted_idx],\n",
    ")\n",
    "plt.xlim(0,2)\n",
    "plt.title(r\"g) f($MSTR$, $H_s$, $T_p$)\", loc='left')\n",
    "# plt.xlabel(\"Permutation importance (non-dim.), test set\")\n",
    "# plt.ylabel(\"Variable\")\n",
    "plt.setp(ax.get_yticklabels (), rotation=30)\n",
    "\n",
    "##################################\n",
    "result = permutation_importance(\n",
    "    model8, X_test8, y_test8, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "ax=plt.subplot(248)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(feature_names8)[sorted_idx],\n",
    ")\n",
    "plt.setp(ax.get_yticklabels (), rotation=30)\n",
    "plt.xlim(0,2)\n",
    "plt.title(r\"h) f(β, $MSTR$, $H_s$, $T_p$)\", loc='left')\n",
    "# plt.xlabel(\"Permutation importance (non-dim.), test set\")\n",
    "# plt.ylabel(\"Variable\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('../model_plots/FourRegions_SEUS_EAus_SCA_wholeAus_d50-8models-featimps-FINAL.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d096e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1414689",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../model_out/dataset_FourRegions_SEUS_EAus_SCA_wholeAus_coast_model1_FINAL.npy', model1out, allow_pickle=True)\n",
    "np.save('../model_out/dataset_FourRegions_SEUS_EAus_SCA_wholeAus_coast_model2_FINAL.npy', model2out, allow_pickle=True)\n",
    "np.save('../model_out/dataset_FourRegions_SEUS_EAus_SCA_wholeAus_coast_model3_FINAL.npy', model3out, allow_pickle=True)\n",
    "np.save('../model_out/dataset_FourRegions_SEUS_EAus_SCA_wholeAus_coast_model4_FINAL.npy', model4out, allow_pickle=True)\n",
    "np.save('../model_out/dataset_FourRegions_SEUS_EAus_SCA_wholeAus_coast_model5_FINAL.npy', model5out, allow_pickle=True)\n",
    "np.save('../model_out/dataset_FourRegions_SEUS_EAus_SCA_wholeAus_coast_model6_FINAL.npy', model6out, allow_pickle=True)\n",
    "np.save('../model_out/dataset_FourRegions_SEUS_EAus_SCA_wholeAus_coast_model7_FINAL.npy', model7out, allow_pickle=True)\n",
    "np.save('../model_out/dataset_FourRegions_SEUS_EAus_SCA_wholeAus_coast_model8_FINAL.npy', model8out, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fcaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model1.get_params()['learning_rate'], model2.get_params()['learning_rate'], model3.get_params()['learning_rate'],\n",
    " model4.get_params()['learning_rate'], model5.get_params()['learning_rate'], model6.get_params()['learning_rate'],\n",
    " model7.get_params()['learning_rate'], model8.get_params()['learning_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model1.get_params()['max_depth'], model2.get_params()['max_depth'], model3.get_params()['max_depth'],\n",
    " model4.get_params()['max_depth'], model5.get_params()['max_depth'], model6.get_params()['max_depth'],\n",
    " model7.get_params()['max_depth'], model8.get_params()['max_depth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model1.get_params()['n_estimators'], model2.get_params()['n_estimators'], model3.get_params()['n_estimators'],\n",
    " model4.get_params()['n_estimators'], model5.get_params()['n_estimators'], model6.get_params()['n_estimators'],\n",
    " model7.get_params()['n_estimators'], model8.get_params()['n_estimators']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model1.get_params()['subsample'], model2.get_params()['subsample'], model3.get_params()['subsample'],\n",
    " model4.get_params()['subsample'], model5.get_params()['subsample'], model6.get_params()['subsample'],\n",
    " model7.get_params()['subsample'], model8.get_params()['subsample']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model1.get_params()['min_samples_split'], model2.get_params()['min_samples_split'], model3.get_params()['min_samples_split'],\n",
    " model4.get_params()['min_samples_split'], model5.get_params()['min_samples_split'], model6.get_params()['min_samples_split'],\n",
    " model7.get_params()['min_samples_split'], model8.get_params()['min_samples_split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model1.get_params()['loss'], model2.get_params()['loss'], model3.get_params()['loss'],\n",
    " model4.get_params()['loss'], model5.get_params()['loss'], model6.get_params()['loss'],\n",
    " model7.get_params()['loss'], model8.get_params()['loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8244e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9992c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "testlim = 100\n",
    "trainlim = 100\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "\n",
    "plt.subplot(2,5,1)\n",
    "plt.hist(X_train5[:,2], bins=np.arange(0,1,.1))\n",
    "plt.ylim(0,trainlim); plt.ylabel('Train set'); plt.xlabel(r'$H_s$ (m)')\n",
    "plt.title(r'a) ', loc='left')\n",
    "\n",
    "plt.subplot(2,5,2)\n",
    "plt.hist(X_train4[:,2], bins=np.arange(3,10,.5))\n",
    "plt.ylim(0,trainlim); plt.xlabel(r'$T_p$ (s)')\n",
    "plt.title(r'b)', loc='left')\n",
    "\n",
    "plt.subplot(2,5,3)\n",
    "plt.hist(X_train6[:,2], bins=np.arange(50,180,10))\n",
    "plt.ylim(0,trainlim); plt.xlabel(r'$\\theta$ (dir.)')\n",
    "plt.title(r'c) ', loc='left')\n",
    "\n",
    "plt.subplot(2,5,4)\n",
    "plt.hist(X_train4[:,3], bins=np.arange(0.2,2.5,.1))\n",
    "plt.ylim(0,trainlim); plt.xlabel('T (m)')\n",
    "plt.title(r'd) ', loc='left')\n",
    "\n",
    "plt.subplot(2,5,5)\n",
    "plt.hist(y_train4, bins=np.arange(0,1,.1))\n",
    "plt.ylim(0,trainlim); plt.xlabel(r'$D_{50}$ (mm)')\n",
    "plt.title(r'e) ', loc='left')\n",
    "\n",
    "plt.subplot(2,5,6)\n",
    "plt.hist(X_test5[:,2], bins=np.arange(0,1,.1))\n",
    "plt.ylim(0,testlim); plt.ylabel('Test set'); plt.xlabel(r'$H_s$ (m)')\n",
    "plt.title(r'f) ', loc='left')\n",
    "\n",
    "plt.subplot(2,5,7)\n",
    "plt.hist(X_test4[:,2], bins=np.arange(3,10,.5))\n",
    "plt.ylim(0,testlim); plt.xlabel(r'$T_p$ (s)')\n",
    "plt.title(r'g)', loc='left')\n",
    "\n",
    "plt.subplot(2,5,8)\n",
    "plt.hist(X_test6[:,2], bins=np.arange(50,180,10))\n",
    "plt.ylim(0,testlim); plt.xlabel(r'$\\theta$ (dir.)')\n",
    "plt.title(r'h) ', loc='left')\n",
    "\n",
    "plt.subplot(2,5,9)\n",
    "plt.hist(X_test4[:,3], bins=np.arange(0.2,2.5,.1))\n",
    "plt.ylim(0,testlim); plt.xlabel('T (m)')\n",
    "plt.title(r'i) ', loc='left')\n",
    "\n",
    "plt.subplot(2,5,10)\n",
    "plt.hist(y_test4, bins=np.arange(0,1,.1))\n",
    "plt.ylim(0,testlim); plt.xlabel('$D_{50}$ (mm)')\n",
    "plt.title(r'j) ', loc='left')\n",
    "\n",
    "plt.savefig('../data_plots/FourRegions_SEUS_EAus_SCA_wholeAus_d50-test_train_dists-FINAL.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490eeff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252039e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
